{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-6 : Self-Practice\n",
    "\n",
    "In this week self-practice, you will implement a neural network model for a regression problem. You will use the [*admission*](./Admission_Predict.csv) dataset attached, used in the previous lab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset and do all the necessary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>316.807500</td>\n",
       "      <td>107.410000</td>\n",
       "      <td>3.087500</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.452500</td>\n",
       "      <td>8.598925</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>0.724350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.473646</td>\n",
       "      <td>6.069514</td>\n",
       "      <td>1.143728</td>\n",
       "      <td>1.006869</td>\n",
       "      <td>0.898478</td>\n",
       "      <td>0.596317</td>\n",
       "      <td>0.498362</td>\n",
       "      <td>0.142609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>8.610000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.062500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GRE Score  TOEFL Score  University Rating         SOP        LOR   \\\n",
       "count  400.000000   400.000000         400.000000  400.000000  400.000000   \n",
       "mean   316.807500   107.410000           3.087500    3.400000    3.452500   \n",
       "std     11.473646     6.069514           1.143728    1.006869    0.898478   \n",
       "min    290.000000    92.000000           1.000000    1.000000    1.000000   \n",
       "25%    308.000000   103.000000           2.000000    2.500000    3.000000   \n",
       "50%    317.000000   107.000000           3.000000    3.500000    3.500000   \n",
       "75%    325.000000   112.000000           4.000000    4.000000    4.000000   \n",
       "max    340.000000   120.000000           5.000000    5.000000    5.000000   \n",
       "\n",
       "             CGPA    Research  Chance of Admit   \n",
       "count  400.000000  400.000000        400.000000  \n",
       "mean     8.598925    0.547500          0.724350  \n",
       "std      0.596317    0.498362          0.142609  \n",
       "min      6.800000    0.000000          0.340000  \n",
       "25%      8.170000    0.000000          0.640000  \n",
       "50%      8.610000    1.000000          0.730000  \n",
       "75%      9.062500    1.000000          0.830000  \n",
       "max      9.920000    1.000000          0.970000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('Admission_Predict.csv')\n",
    "\n",
    "df = df.drop(['Serial No.'], axis=1)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y  = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create custom pytorch `Dataset`\n",
    "\n",
    "You should create a class `CustomDataset` that inherits  the abstract class `torch.utils.data.Dataset` from pytorch. \n",
    "\n",
    "> **Note** You should overwrite `__getitem__`, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite `__len__`, which is expected to return the size of the dataset by many `~torch.utils.data.Sampler` implementations and the default options of `~torch.utils.data.DataLoader`.\n",
    "\n",
    "#### Split your dataset into train and test data loaders\n",
    "You can create a `CustomDataset` instance with the entire dataframe and use [`random_split`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) to split it into training and testing datasets. And then, create test and train dataloader. Or you can split using `train_test_split` from sklearn and past the splitted sets to your Custom dataset class. \n",
    "\n",
    "Create train and test dataloader with `batch_size = 32` each complete the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustumData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        #initaliaze the variable\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the len of the dataset\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # return a tuple samples and labels with the corresponding index idx\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets\n",
    "train_dataset = CustumData(X_train, y_train)\n",
    "test_dataset = CustumData(X_test, y_test) \n",
    "\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = next(iter(train_dataloader))\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the model\n",
    "\n",
    "Using `nn`, Create a neural network with 1 hidden layers of size 100, each must be followed by a `leaky_relu` activation function and define the forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# complete the code\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_hidden_unit = 100):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, n_hidden_unit)\n",
    "        self.fc2 = nn.Linear(n_hidden_unit, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Net(n_hidden_unit = 100).to(device)\n",
    "#model = nn.Sequential(nn.Linear(7, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training loop\n",
    "\n",
    "Define the appropriate loss function and the training loop for the training and the testing dataloader (as done in the lab). Use SGD optimizer with learning rate 0.01 and momentum 0.5\n",
    "\n",
    "Print the final loss on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 2\n",
    "\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "    \n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/320 (0%)]\tLoss: 0.003071\n",
      "Train Epoch: 1 [64/320 (20%)]\tLoss: 0.001901\n",
      "Train Epoch: 1 [128/320 (40%)]\tLoss: 0.003411\n",
      "Train Epoch: 1 [192/320 (60%)]\tLoss: 0.002080\n",
      "Train Epoch: 1 [256/320 (80%)]\tLoss: 0.001416\n",
      "\n",
      "Test set: Average loss: 0.00019697147654369475\n",
      "\n",
      "Train Epoch: 2 [0/320 (0%)]\tLoss: 0.003001\n",
      "Train Epoch: 2 [64/320 (20%)]\tLoss: 0.001692\n",
      "Train Epoch: 2 [128/320 (40%)]\tLoss: 0.002119\n",
      "Train Epoch: 2 [192/320 (60%)]\tLoss: 0.001976\n",
      "Train Epoch: 2 [256/320 (80%)]\tLoss: 0.002525\n",
      "\n",
      "Test set: Average loss: 0.0001965737814316526\n",
      "\n",
      "Train Epoch: 3 [0/320 (0%)]\tLoss: 0.003997\n",
      "Train Epoch: 3 [64/320 (20%)]\tLoss: 0.001100\n",
      "Train Epoch: 3 [128/320 (40%)]\tLoss: 0.003419\n",
      "Train Epoch: 3 [192/320 (60%)]\tLoss: 0.002432\n",
      "Train Epoch: 3 [256/320 (80%)]\tLoss: 0.003404\n",
      "\n",
      "Test set: Average loss: 0.00020355077576823534\n",
      "\n",
      "Train Epoch: 4 [0/320 (0%)]\tLoss: 0.001913\n",
      "Train Epoch: 4 [64/320 (20%)]\tLoss: 0.002287\n",
      "Train Epoch: 4 [128/320 (40%)]\tLoss: 0.004065\n",
      "Train Epoch: 4 [192/320 (60%)]\tLoss: 0.003431\n",
      "Train Epoch: 4 [256/320 (80%)]\tLoss: 0.002709\n",
      "\n",
      "Test set: Average loss: 0.00019874173012794926\n",
      "\n",
      "Train Epoch: 5 [0/320 (0%)]\tLoss: 0.003059\n",
      "Train Epoch: 5 [64/320 (20%)]\tLoss: 0.002529\n",
      "Train Epoch: 5 [128/320 (40%)]\tLoss: 0.001129\n",
      "Train Epoch: 5 [192/320 (60%)]\tLoss: 0.003509\n",
      "Train Epoch: 5 [256/320 (80%)]\tLoss: 0.003750\n",
      "\n",
      "Test set: Average loss: 0.00019545836839824916\n",
      "\n",
      "Train Epoch: 6 [0/320 (0%)]\tLoss: 0.001727\n",
      "Train Epoch: 6 [64/320 (20%)]\tLoss: 0.003998\n",
      "Train Epoch: 6 [128/320 (40%)]\tLoss: 0.001445\n",
      "Train Epoch: 6 [192/320 (60%)]\tLoss: 0.001676\n",
      "Train Epoch: 6 [256/320 (80%)]\tLoss: 0.002851\n",
      "\n",
      "Test set: Average loss: 0.00019848575320793315\n",
      "\n",
      "Train Epoch: 7 [0/320 (0%)]\tLoss: 0.002270\n",
      "Train Epoch: 7 [64/320 (20%)]\tLoss: 0.000973\n",
      "Train Epoch: 7 [128/320 (40%)]\tLoss: 0.001549\n",
      "Train Epoch: 7 [192/320 (60%)]\tLoss: 0.002044\n",
      "Train Epoch: 7 [256/320 (80%)]\tLoss: 0.002372\n",
      "\n",
      "Test set: Average loss: 0.00019582920940592886\n",
      "\n",
      "Train Epoch: 8 [0/320 (0%)]\tLoss: 0.002692\n",
      "Train Epoch: 8 [64/320 (20%)]\tLoss: 0.001707\n",
      "Train Epoch: 8 [128/320 (40%)]\tLoss: 0.002353\n",
      "Train Epoch: 8 [192/320 (60%)]\tLoss: 0.001489\n",
      "Train Epoch: 8 [256/320 (80%)]\tLoss: 0.001640\n",
      "\n",
      "Test set: Average loss: 0.0001927310193423182\n",
      "\n",
      "Train Epoch: 9 [0/320 (0%)]\tLoss: 0.002319\n",
      "Train Epoch: 9 [64/320 (20%)]\tLoss: 0.002662\n",
      "Train Epoch: 9 [128/320 (40%)]\tLoss: 0.002116\n",
      "Train Epoch: 9 [192/320 (60%)]\tLoss: 0.003464\n",
      "Train Epoch: 9 [256/320 (80%)]\tLoss: 0.002861\n",
      "\n",
      "Test set: Average loss: 0.00019821891037281604\n",
      "\n",
      "Train Epoch: 10 [0/320 (0%)]\tLoss: 0.004283\n",
      "Train Epoch: 10 [64/320 (20%)]\tLoss: 0.001645\n",
      "Train Epoch: 10 [128/320 (40%)]\tLoss: 0.001018\n",
      "Train Epoch: 10 [192/320 (60%)]\tLoss: 0.003353\n",
      "Train Epoch: 10 [256/320 (80%)]\tLoss: 0.001819\n",
      "\n",
      "Test set: Average loss: 0.00019936430617235602\n",
      "\n",
      "Train Epoch: 11 [0/320 (0%)]\tLoss: 0.004474\n",
      "Train Epoch: 11 [64/320 (20%)]\tLoss: 0.001752\n",
      "Train Epoch: 11 [128/320 (40%)]\tLoss: 0.001404\n",
      "Train Epoch: 11 [192/320 (60%)]\tLoss: 0.003236\n",
      "Train Epoch: 11 [256/320 (80%)]\tLoss: 0.001946\n",
      "\n",
      "Test set: Average loss: 0.0001977500505745411\n",
      "\n",
      "Train Epoch: 12 [0/320 (0%)]\tLoss: 0.002129\n",
      "Train Epoch: 12 [64/320 (20%)]\tLoss: 0.003427\n",
      "Train Epoch: 12 [128/320 (40%)]\tLoss: 0.002707\n",
      "Train Epoch: 12 [192/320 (60%)]\tLoss: 0.001637\n",
      "Train Epoch: 12 [256/320 (80%)]\tLoss: 0.001343\n",
      "\n",
      "Test set: Average loss: 0.00020020983938593418\n",
      "\n",
      "Train Epoch: 13 [0/320 (0%)]\tLoss: 0.002084\n",
      "Train Epoch: 13 [64/320 (20%)]\tLoss: 0.001503\n",
      "Train Epoch: 13 [128/320 (40%)]\tLoss: 0.002768\n",
      "Train Epoch: 13 [192/320 (60%)]\tLoss: 0.002114\n",
      "Train Epoch: 13 [256/320 (80%)]\tLoss: 0.003294\n",
      "\n",
      "Test set: Average loss: 0.00019390174420550466\n",
      "\n",
      "Train Epoch: 14 [0/320 (0%)]\tLoss: 0.003743\n",
      "Train Epoch: 14 [64/320 (20%)]\tLoss: 0.001224\n",
      "Train Epoch: 14 [128/320 (40%)]\tLoss: 0.003712\n",
      "Train Epoch: 14 [192/320 (60%)]\tLoss: 0.001520\n",
      "Train Epoch: 14 [256/320 (80%)]\tLoss: 0.002277\n",
      "\n",
      "Test set: Average loss: 0.00019871890835929663\n",
      "\n",
      "Train Epoch: 15 [0/320 (0%)]\tLoss: 0.001643\n",
      "Train Epoch: 15 [64/320 (20%)]\tLoss: 0.001480\n",
      "Train Epoch: 15 [128/320 (40%)]\tLoss: 0.003353\n",
      "Train Epoch: 15 [192/320 (60%)]\tLoss: 0.002825\n",
      "Train Epoch: 15 [256/320 (80%)]\tLoss: 0.002428\n",
      "\n",
      "Test set: Average loss: 0.00019451639091130346\n",
      "\n",
      "Train Epoch: 16 [0/320 (0%)]\tLoss: 0.002197\n",
      "Train Epoch: 16 [64/320 (20%)]\tLoss: 0.001493\n",
      "Train Epoch: 16 [128/320 (40%)]\tLoss: 0.002999\n",
      "Train Epoch: 16 [192/320 (60%)]\tLoss: 0.002538\n",
      "Train Epoch: 16 [256/320 (80%)]\tLoss: 0.002106\n",
      "\n",
      "Test set: Average loss: 0.00019887235685018824\n",
      "\n",
      "Train Epoch: 17 [0/320 (0%)]\tLoss: 0.001946\n",
      "Train Epoch: 17 [64/320 (20%)]\tLoss: 0.003150\n",
      "Train Epoch: 17 [128/320 (40%)]\tLoss: 0.002929\n",
      "Train Epoch: 17 [192/320 (60%)]\tLoss: 0.001977\n",
      "Train Epoch: 17 [256/320 (80%)]\tLoss: 0.002597\n",
      "\n",
      "Test set: Average loss: 0.0002030903473496437\n",
      "\n",
      "Train Epoch: 18 [0/320 (0%)]\tLoss: 0.002453\n",
      "Train Epoch: 18 [64/320 (20%)]\tLoss: 0.001509\n",
      "Train Epoch: 18 [128/320 (40%)]\tLoss: 0.001763\n",
      "Train Epoch: 18 [192/320 (60%)]\tLoss: 0.001533\n",
      "Train Epoch: 18 [256/320 (80%)]\tLoss: 0.001384\n",
      "\n",
      "Test set: Average loss: 0.0001960262467036955\n",
      "\n",
      "Train Epoch: 19 [0/320 (0%)]\tLoss: 0.002073\n",
      "Train Epoch: 19 [64/320 (20%)]\tLoss: 0.001537\n",
      "Train Epoch: 19 [128/320 (40%)]\tLoss: 0.001761\n",
      "Train Epoch: 19 [192/320 (60%)]\tLoss: 0.002233\n",
      "Train Epoch: 19 [256/320 (80%)]\tLoss: 0.002175\n",
      "\n",
      "Test set: Average loss: 0.0001961533824214712\n",
      "\n",
      "Train Epoch: 20 [0/320 (0%)]\tLoss: 0.002505\n",
      "Train Epoch: 20 [64/320 (20%)]\tLoss: 0.001744\n",
      "Train Epoch: 20 [128/320 (40%)]\tLoss: 0.001852\n",
      "Train Epoch: 20 [192/320 (60%)]\tLoss: 0.001733\n",
      "Train Epoch: 20 [256/320 (80%)]\tLoss: 0.002002\n",
      "\n",
      "Test set: Average loss: 0.00019913600553991273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nn = model(torch.tensor(X_test).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare your Neural network model to a Linear Regression\n",
    "Train a simple linear regression model on the training set and print MSE on the testing set (`X_test`). Also print the MSE on the test set using the your neural model. \n",
    "\n",
    "> Compare the results (which performs best) and justify why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.004617003377285012\n",
      "Neural Network MSE: 0.006021902921991413\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Linear Regression MSE: {mse_lr}\")\n",
    "print(f\"Neural Network MSE: {mse_nn}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
