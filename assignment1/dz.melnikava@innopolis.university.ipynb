{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Machine Learning Assignment 1 - Instructions & Guidelines\n",
    "\n",
    "### **ðŸ“ General Guidelines**\n",
    "Welcome to Machine Learning Assignment 1! This assignment will test your understanding of **regression and classification models**, including **data preprocessing, hyperparameter tuning, and model evaluation**.\n",
    "\n",
    "Follow the instructions carefully, and ensure your implementation is **correct, well-structured, and efficient**.\n",
    "\n",
    "ðŸ”¹ **Submission Format:**  \n",
    "- Your submission **must be a single Jupyter Notebook (.ipynb)** file.  \n",
    "- **File Naming Convention:**  \n",
    "  - Use **your university email as the filename**, e.g.,  \n",
    "    ```\n",
    "    j.doe@innopolis.university.ipynb\n",
    "    ```\n",
    "  - **Do NOT modify this format**, or your submission may not be graded.\n",
    "\n",
    "ðŸ”¹ **Assignment Breakdown:**\n",
    "| Task | Description | Points |\n",
    "|------|------------|--------|\n",
    "| **Task 1.1** | Linear Regression | 20 |\n",
    "| **Task 1.2** | Polynomial Regression | 20 |\n",
    "| **Task 2.1** | Data Preprocessing | 15 |\n",
    "| **Task 2.2** | Model Comparison | 45 |\n",
    "| **Total** | - | **100** |\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“‚ Dataset & Assumptions**\n",
    "The dataset files are stored in the `datasets/` folder.  \n",
    "- **Regression Dataset:** `datasets/task1_data.csv`\n",
    "- **Classification Dataset:** `datasets/pokemon_modified.csv`\n",
    "\n",
    "Each dataset is structured as follows:\n",
    "\n",
    "ðŸ”¹ **`task1_data.csv` (for regression tasks)**  \n",
    "- Contains `X_train`, `y_train`, `X_test`, and `y_test`.  \n",
    "- The goal is to fit **linear and polynomial regression models** and evaluate their performance.  \n",
    "\n",
    "ðŸ”¹ **`pokemon_modified.csv` (for classification tasks)**  \n",
    "- Contains PokÃ©mon attributes, with `is_legendary` as the **binary target variable (0 or 1)**.  \n",
    "- Some features contain **missing values** and **categorical variables**, requiring preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸš€ How to Approach the Assignment**\n",
    "1. **Start with Regression (Task 1)**\n",
    "   - Implement **linear regression** and **polynomial regression**.\n",
    "   - Use **GridSearchCV** for polynomial regression to find the best degree.\n",
    "   - Evaluate using **MSE, RMSE, MAE, and RÂ² Score**.\n",
    "\n",
    "2. **Move to Data Preprocessing (Task 2.1)**\n",
    "   - Load and clean the PokÃ©mon dataset.\n",
    "   - Handle **missing values** correctly.\n",
    "   - Encode categorical variables properly.\n",
    "   - Ensure **no data leakage** when doing the preprocessing.\n",
    "\n",
    "3. **Train and Evaluate Classification Models (Task 2.2)**\n",
    "   - Train **Logistic Regression, KNN, and Naive Bayes**.\n",
    "   - Use **GridSearchCV** for hyperparameter tuning.\n",
    "   - Evaluate models using **Accuracy, Precision, Recall, and F1-score**.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Grading & Evaluation**\n",
    "- Your notebook will be **autograded**, so ensure:\n",
    "  - Your function names **exactly match** the given specifications.\n",
    "  - Your output format matches the expected results.\n",
    "- Partial credit will be given where applicable.\n",
    "\n",
    "ðŸ”¹ **Need Help?**  \n",
    "- If you have any questions, refer to the **assignment markdown instructions** in each task before asking for clarifications.\n",
    "- You can post your question on this [Google sheet](https://docs.google.com/spreadsheets/d/1oyrqXDjT2CeGYx12aZhZ-oDKcQQ-PCgT91wHPhTlBCY/edit?usp=sharing)\n",
    "\n",
    "ðŸš€ **Good luck! Happy coding!** ðŸŽ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ\n",
    "\n",
    "**1) Should we include the lines to import the libraries?**\n",
    "\n",
    "- **Answer:**  \n",
    "  It doesn't matter if you include extra import lines, as the grader will only call the specified functions.\n",
    "\n",
    "**2) Is it okay to submit my file with code outside of the functions?**\n",
    "\n",
    "- **Answer:**  \n",
    "  Yes, you can include additional code outside of the functions as long as the entire script runs correctly when converted to a `.py` file.\n",
    "\n",
    "**Important Clarification:**\n",
    "\n",
    "- The grader will first convert the Jupyter Notebook (.ipynb) into a Python file (.py) and then run it.\n",
    "- **Note:** Please do not include any commands like `!pip install numpy` because they may break the conversion process and therefore the submission will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear and Polynomial Regression (30 Points)\n",
    "\n",
    "### Task 1.1 - Linear Regression (15 Points)\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
    "2. Extract training and testing data from the following columns:\n",
    "   - `\"X_train\"`: Training feature values.\n",
    "   - `\"y_train\"`: Training target values.\n",
    "   - `\"X_test\"`: Testing feature values.\n",
    "   - `\"y_test\"`: Testing target values.\n",
    "3. Train a **linear regression model** on `X_train` and `y_train`.\n",
    "4. Use the trained model to predict `y_test` values.\n",
    "5. Compute and return the following **evaluation metrics** as a dictionary:\n",
    "   - **Mean Squared Error (MSE)**\n",
    "   - **Root Mean Squared Error (RMSE)**\n",
    "   - **Mean Absolute Error (MAE)**\n",
    "   - **RÂ² Score**\n",
    "6. The function signature should match:\n",
    "   ```python\n",
    "   def task1_linear_regression() -> Dict[str, float]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do not use any other libraries except for the ones imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import importlib.util\n",
    "import nbformat\n",
    "from tempfile import NamedTemporaryFile\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(f'datasets/{name}.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/task1_data.csv', sep=',')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['X_test'].std())\n",
    "# print(df['y_test'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = df['X_train'].values.reshape(-1, 1), df['y_train'], df['X_test'].values.reshape(-1, 1), df['y_test']\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = linear_model.predict(x_test)\n",
    "\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.scatter(x_train, y_train, color='blue', label='Training Data')\n",
    "# plt.scatter(x_test, y_test, color='red', label = 'Testing Data')\n",
    "# plt.plot(x_test, y_pred, color='green', label='Linear Regression')\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('y')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = {\n",
    "#     \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "#     \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "#     \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "#     \"R2\": r2_score(y_test, y_pred)\n",
    "# }\n",
    "\n",
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_linear_regression() -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Performs linear regression on a predefined dataset and returns performance metrics.\n",
    "\n",
    "    **Dataset Assumption:**\n",
    "    - The dataset is located at `\"datasets/task1_data.csv\"`.\n",
    "    - It should contain the following columns:\n",
    "      - `\"X_train\"`: Training feature values (numerical).\n",
    "      - `\"y_train\"`: Training target values.\n",
    "      - `\"X_test\"`: Testing feature values (numerical).\n",
    "      - `\"y_test\"`: Testing target values.\n",
    "\n",
    "    **Process:**\n",
    "    1. Load the dataset from `\"datasets/task1_data.csv\"`.\n",
    "    2. Extract training and testing data.\n",
    "    3. Train a linear regression model on `X_train, y_train`.\n",
    "    4. Use the trained model to predict `y_test` values.\n",
    "    5. Compute evaluation metrics: **MSE, RMSE, MAE, RÂ² Score**.\n",
    "\n",
    "    **Output (Dictionary with Regression Metrics):**\n",
    "    ```python\n",
    "    {\n",
    "        \"MSE\": <Mean Squared Error>,\n",
    "        \"RMSE\": <Root Mean Squared Error>,\n",
    "        \"MAE\": <Mean Absolute Error>,\n",
    "        \"R2\": <RÂ² Score>\n",
    "    }\n",
    "    ```\n",
    "    \"\"\"\n",
    "    df = load_dataset('task1_data')\n",
    "\n",
    "    x_train, y_train, x_test, y_test = df['X_train'].values.reshape(-1, 1), df['y_train'], df['X_test'].values.reshape(-1, 1), df['y_test']\n",
    "\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = linear_model.predict(x_test)\n",
    "\n",
    "    return {\n",
    "    \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "    \"R2\": r2_score(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1_linear_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Polynomial Regression (15 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
    "2. Extract training and testing data from the following columns:\n",
    "   - `\"X_train\"`: Training feature values.\n",
    "   - `\"y_train\"`: Training target values.\n",
    "   - `\"X_test\"`: Testing feature values.\n",
    "   - `\"y_test\"`: Testing target values.\n",
    "3. Define a **pipeline** that includes:\n",
    "   - **Polynomial feature transformation** (degree range: **2 to 10**).\n",
    "   - **Linear regression model**.\n",
    "4. Use **GridSearchCV** with **8-fold cross-validation** to determine the best polynomial degree.\n",
    "5. Train the model with the best polynomial degree and **evaluate it on the test set**.\n",
    "6. Compute and return the following results as a dictionary:\n",
    "   - **Best polynomial degree** (`best_degree`)\n",
    "   - **Mean Squared Error (MSE)**\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task1_polynomial_regression() -> Dict[str, float]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_polynomial_regression() -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Performs polynomial regression using GridSearchCV to find the best polynomial degree.\n",
    "\n",
    "\n",
    "    **Process:**\n",
    "    1. Load the dataset and extract `X_train, y_train, X_test, y_test`.\n",
    "    2. Define a **pipeline** with polynomial feature transformation and linear regression.\n",
    "    3. Use **GridSearchCV** (with 8-fold cross-validation) to determine the best polynomial degree (range: **2 to 10**).\n",
    "    4. Train the best polynomial regression model and evaluate its performance.\n",
    "    5. Compute and return:\n",
    "       - **Best polynomial degree (`best_degree`)**\n",
    "       - **Mean Squared Error (MSE)**\n",
    "\n",
    "     **Expected Output:**\n",
    "    ```\n",
    "    {\n",
    "        \"best_degree\": <Optimal Polynomial Degree>,\n",
    "        \"MSE\": <Mean Squared Error>\n",
    "    }\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    df = load_dataset('task1_data')\n",
    "\n",
    "    x_train, y_train, x_test, y_test = df['X_train'].values.reshape(-1, 1), df['y_train'], df['X_test'].values.reshape(-1, 1), df['y_test']\n",
    "\n",
    "    pipeline = Pipeline([('poly', PolynomialFeatures()), ('linear_model', LinearRegression())])\n",
    "    \n",
    "    param_grid = {'poly__degree': range(2, 11)}\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=8, scoring='neg_mean_squared_error')\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_degree = grid_search.best_params_['poly__degree']\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "    MSE = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        'best_degree': best_degree,\n",
    "        'MSE': MSE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1_polynomial_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Classification with Data Preprocessing (70 Points)\n",
    "\n",
    "### Task 2.1 - Data Preprocessing (30 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/pokemon_modified.csv`**.\n",
    "2. Look at the data and study the provided features\n",
    "3. Remove the **two redundant features**\n",
    "4. Handle **missing values**:\n",
    "   - Use **mean imputation** for **\"height_m\"** and **\"weight_kg\"**.\n",
    "   - Use **median imputation** for **\"percentage_male\"**.\n",
    "5. Perform **one-hot encoding** for the categorical column **\"type1\"**.\n",
    "6. Ensure the **target variable** (`\"is_legendary\"`) is present.\n",
    "7. **Split the data into training and testing sets** (`80%-20%` split). Is it balanced?\n",
    "8. **Apply feature scaling** using **StandardScaler** or **MinMaxScaler**.\n",
    "9. Return the following:\n",
    "   - `X_train_scaled`: Processed training features.\n",
    "   - `X_test_scaled`: Processed testing features.\n",
    "   - `y_train`: Training labels.\n",
    "   - `y_test`: Testing labels.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('pokemon_modified')\n",
    "\n",
    "# df.columns.get_loc('classification')\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_redundant_features(dataset: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    return dataset.drop(dataset.columns[cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    imputer_height_weight = SimpleImputer(strategy='mean')\n",
    "    dataset[['height_m', 'weight_kg']] = imputer_height_weight.fit_transform(dataset[['height_m', 'weight_kg']])\n",
    "\n",
    "\n",
    "    imputer_percentage_male = SimpleImputer(strategy='median')\n",
    "    dataset[['percentage_male']] = imputer_percentage_male.fit_transform(dataset[['percentage_male']])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_new_features(dataset: pd.DataFrame, feature_name) -> pd.DataFrame:\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_type1 = encoder.fit_transform(dataset[feature_name])\n",
    "\n",
    "    new_cols = pd.DataFrame(encoded_type1, columns=encoder.get_feature_names_out(feature_name))\n",
    "    dataset = pd.concat([dataset, new_cols], axis=1)\n",
    "\n",
    "    dataset.drop(feature_name, axis=1, inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Preprocesses the PokÃ©mon dataset by handling missing values, encoding categorical data, \n",
    "    and applying feature scaling before returning train-test splits, ensuring class balance.\n",
    "\n",
    "    **Dataset Assumption:**\n",
    "    - The dataset is located at `\"datasets/pokemon_modified.csv\"`.\n",
    "\n",
    "    **Process:**\n",
    "    1. Load the dataset and remove redundant columns.\n",
    "    2. Handle missing values:\n",
    "       - Mean imputation for **\"height_m\"** and **\"weight_kg\"**.\n",
    "       - Median imputation for **\"percentage_male\"**.\n",
    "    3. Perform **one-hot encoding** on `\"type1\"`.\n",
    "    4. Ensure **\"is_legendary\"** is present as the target variable.\n",
    "    5. Split the dataset into **80% training, 20% testing** using **stratification** to maintain class balance.\n",
    "    6. Apply feature scaling (**StandardScaler**).\n",
    "    7. Return the preprocessed train-test splits.\n",
    "    \"\"\"\n",
    "\n",
    "    df = load_dataset('pokemon_modified')\n",
    "    \n",
    "    columns = [df.columns.get_loc('classification'), df.columns.get_loc('name')]\n",
    "\n",
    "    df = delete_redundant_features(df, columns)\n",
    "\n",
    "    df = handle_missing_values(df)\n",
    "\n",
    "    df = ohe_new_features(df, ['type1'])\n",
    "\n",
    "    data_label = df['is_legendary']\n",
    "    data_feature = df.drop(['is_legendary'], axis=1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_feature, data_label, test_size=0.2, random_state=42, stratify=data_label)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(x_train)\n",
    "\n",
    "    x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)\n",
    "    x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
    "    \n",
    "    \n",
    "    return [x_train, x_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - Model Comparison (40 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. **Train three classification models** on the preprocessed dataset:\n",
    "   - **Logistic Regression**\n",
    "   - **K-Nearest Neighbors (KNN)**\n",
    "   - **Gaussian Naive Bayes (GNB)**\n",
    "2. Use **GridSearchCV** for **hyperparameter tuning** on:\n",
    "   - **Logistic Regression**: Regularization strength (`C`) and penalty (`l1`, `l2`).\n",
    "   - **KNN**: Number of neighbors (`n_neighbors`), weight function, and distance metric.\n",
    "3. Train each model on the **training set** and evaluate on the **test set**.\n",
    "4. Compute the following **evaluation metrics**:\n",
    "   - **Accuracy**\n",
    "   - **Precision**\n",
    "   - **Recall**\n",
    "   - **F1 Score**\n",
    "5. Return a dictionary containing the evaluation metrics for each model.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task2_model_comparison() -> Dict[str, Dict[str, float]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def optimize_threshold_gnb(x_train, x_test, y_train, y_test, thresholds):\n",
    "#         gnb = GaussianNB()\n",
    "#         gnb.fit(x_train, y_train)\n",
    "#         best_precision = 0\n",
    "#         best_recall = 0\n",
    "#         best_f1 = 0\n",
    "#         best_accuracy = 0\n",
    "\n",
    "#         for threshold in thresholds:\n",
    "#             y_proba = gnb.predict_proba(x_test)[:, 1]\n",
    "#             y_pred_threshold = (y_proba >= threshold).astype(int)\n",
    "\n",
    "#             precision = precision_score(y_test, y_pred_threshold)\n",
    "#             recall = recall_score(y_test, y_pred_threshold)\n",
    "#             f1 = f1_score(y_test, y_pred_threshold)\n",
    "#             accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "\n",
    "#             if precision > best_precision:\n",
    "#                 best_precision = precision\n",
    "#                 best_recall = recall\n",
    "#                 best_f1 = f1\n",
    "#                 best_accuracy = accuracy\n",
    "\n",
    "#         return best_precision, best_recall, best_f1, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_model_comparison() -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Trains and evaluates three classification models using GridSearchCV for hyperparameter tuning.\n",
    "\n",
    "    **Dataset Assumption:**\n",
    "    - The preprocessed dataset is obtained from `task2_preprocessing()`, which returns:\n",
    "      - `X_train`: Training features (scaled)\n",
    "      - `X_test`: Testing features (scaled)\n",
    "      - `y_train`: Training labels\n",
    "      - `y_test`: Testing labels\n",
    "\n",
    "    **Process:**\n",
    "    1. Load the preprocessed dataset from `task2_preprocessing()`.\n",
    "    2. Train the following models:\n",
    "       - **Logistic Regression** (Hyperparameters: `C`, `penalty`, `solver`).\n",
    "       - **K-Nearest Neighbors (KNN)** (Hyperparameters: `n_neighbors`, `weights`, `metric`).\n",
    "       - **Gaussian Naive Bayes** (No hyperparameter tuning required).\n",
    "    3. Evaluate the models using the following metrics:\n",
    "       - **Accuracy**\n",
    "       - **Precision**\n",
    "       - **Recall**\n",
    "       - **F1 Score**\n",
    "    4. Return a dictionary with model names as keys and evaluation metrics as values.\n",
    "\n",
    "    **Expected Output:**\n",
    "    ```python\n",
    "    {\n",
    "        \"Logistic Regression\": {\"accuracy\": <float>, \"precision\": <float>, \"recall\": <float>, \"f1_score\": <float>},\n",
    "        \"KNN\": {\"accuracy\": <float>, \"precision\": <float>, \"recall\": <float>, \"f1_score\": <float>},\n",
    "        \"Naive Bayes\": {\"accuracy\": <float>, \"precision\": <float>, \"recall\": <float>, \"f1_score\": <float>}\n",
    "    }\n",
    "    ```\n",
    "    \"\"\"\n",
    "    x_train, x_test, y_train, y_test = task2_preprocessing()\n",
    "\n",
    "    models = {\n",
    "        'Logistic Regression': {\n",
    "            'estimator': LogisticRegression(),\n",
    "            'params': {\n",
    "                'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear']\n",
    "            }\n",
    "        },\n",
    "        'KNN': {\n",
    "            'estimator': KNeighborsClassifier(),\n",
    "            'params': {\n",
    "                'n_neighbors': list(range(2,20)),\n",
    "                'weights': ['distance'],\n",
    "                'metric': ['euclidean', 'manhattan']\n",
    "            }\n",
    "        },\n",
    "        'Naive Bayes': {\n",
    "            'estimator': GaussianNB(),\n",
    "            'params': {\n",
    "                'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model_info in models.items():\n",
    "        estimator = model_info['estimator']\n",
    "        params = model_info['params']\n",
    "\n",
    "        grid_search = GridSearchCV(estimator, params, cv=8, scoring='accuracy')\n",
    "        grid_search.fit(x_train, y_train)\n",
    "\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "\n",
    "        y_pred = best_estimator.predict(x_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # if model_name == 'Naive Bayes':\n",
    "        #     thresholds = np.arange(0.1, 1, 0.05)\n",
    "        #     best_precision, best_recall, best_f1, best_accuracy = optimize_threshold_gnb(x_train, x_test, y_train, y_test, thresholds)\n",
    "        #     results[model_name] = {\n",
    "        #         'accuracy': best_accuracy,\n",
    "        #         'precision': best_precision,\n",
    "        #         'recall': best_recall,\n",
    "        #         'f1_score': best_f1\n",
    "        #     }\n",
    "        # else:\n",
    "        results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'accuracy': 0.9875776397515528,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.8571428571428571,\n",
       "  'f1_score': 0.9230769230769231},\n",
       " 'KNN': {'accuracy': 0.9627329192546584,\n",
       "  'precision': 0.9,\n",
       "  'recall': 0.6428571428571429,\n",
       "  'f1_score': 0.75},\n",
       " 'Naive Bayes': {'accuracy': 0.9565217391304348,\n",
       "  'precision': 0.8181818181818182,\n",
       "  'recall': 0.6428571428571429,\n",
       "  'f1_score': 0.72}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task2_model_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
