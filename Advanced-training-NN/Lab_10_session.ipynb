{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ84QHlub057"
   },
   "source": [
    "# Lab-10:  Advanced training of Neural Networks in Pytorch\n",
    "\n",
    "You've already trained pretty efficient and precise neural networks. However, there are some methods that can make the training faster and more reliable. In this lab you're going to familiarize yourself with such techniques.\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "Learn how to use:\n",
    "1. Data augmentation; \n",
    "2. Batch normalization;\n",
    "3. Dropout;\n",
    "4. Early stopping;\n",
    "5. Learning rate scheduling;\n",
    "6. Tensorboard for training control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyAmrz7Hb06D",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1) Data augmentation\n",
    "We're going to use a new dataset, CIFAR10, as our example for the task of classification. The data set consists of 60000 32x32 color images in 10 classes looking like this:\n",
    "\n",
    "![example](https://pytorch.org/tutorials/_images/cifar10.png)\n",
    "\n",
    "**Question: why might we use data augmentation? What problem does it solve?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.**\n",
    "\n",
    "**1) Write the following augmentation transforms of CIFAR10 dataset:**\n",
    "\n",
    "    - Crop (with size = 32, crop = 2) \n",
    "    - Horizontal flip (with probability 0.5)\n",
    "    - Rotation (with 10 degrees max) \n",
    "    - Random affine (degrees = 0, shear = 10, scale=(0.8,1.2))\n",
    "    \n",
    "**2) Define DataLoader-s of CIFAR-10 with these transforms**\n",
    "\n",
    "**Hint:** refer [ILLUSTRATION OF TRANSFORMS](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py) in Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8K1MNlrdb06D",
    "outputId": "d21cb3ea-2859-48fe-fef3-a131231264e1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# You can increase these values if you've enough computational power\n",
    "train_batch_size = 128\n",
    "test_batch_size = 128\n",
    "\n",
    "\n",
    "# Put augmentations\n",
    "train_transforms = transforms.Compose([\n",
    "    ...,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "# Do not modify test transforms, because it will corrupt test data\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "# You should get how to get the training part of datasets.CIFAR10 and\n",
    "# how to apply train_transforms here\n",
    "# Specify path of downloaded set in root, if you've loaded it\n",
    "train_dataset = datasets.CIFAR10(root='cifar10', ...)\n",
    "\n",
    "# Define the loader of train data set: use train_batch_size as batch_size\n",
    "# and don't forget to shuffle\n",
    "train_data_loader = data.DataLoader(...)\n",
    "\n",
    "\n",
    "# Do the same for test part of CIFAR10, apply test_transforms\n",
    "test_dataset = datasets.CIFAR10(root='cifar10', ...)\n",
    "\n",
    "# Define the loader of test data set: use test_batch_size\n",
    "test_data_loader = data.DataLoader(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of transformations\n",
    "import matplotlib.pyplot as plt\n",
    "images, _ = next(iter(train_data_loader))\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4)\n",
    "\n",
    "for i in range(4):\n",
    "    ax = axs[i]\n",
    "    ax.imshow(images[i].numpy().transpose(1,2,0))\n",
    "    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH8Ovp-Sb06E"
   },
   "source": [
    "# 2) Build a model with dropout and batch normalization\n",
    "\n",
    "Here we're going to build about the same model as we used before, but with two new layers: batch normalization and dropout.\n",
    "\n",
    "**Question: what's the purpose of these operations? What's the proposed order of their disposition relative to other layers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2. Declare 4 blocks (nn.Sequential) of Custom model with the default parameters (unless otherwise stated):**\n",
    "\n",
    "**1st block, convolutional)**\n",
    "\n",
    "    - Convolution layer with 16 filters, kernel size equal to 3x3 and stride 1x1. Use ReLU as activation;\n",
    "    - Max pool layer with kernel size 2;\n",
    "    - Batch norm layer.\n",
    "    \n",
    "**2nd block, convolutional)**\n",
    "\n",
    "    - Convolution layer with 32 filters, kernel size equal to 3x3 and stride 1x1. Use ReLU as activation;\n",
    "    - Batch norm layer;\n",
    "    - Dropout layer with probability of unit drop equal to 0.25.\n",
    "    \n",
    "**3rd block, convolutional)**\n",
    "\n",
    "    - Convolution layer with 64 filters, kernel size equal to 3x3 and stride 1x1. Use ReLU as activation;\n",
    "    - Batch norm layer;\n",
    "\n",
    "**4th block, linear)**\n",
    "\n",
    "    - Linear layer. If you stated the previous parameters properly, in_features should be 64*11*11. Set out_features as 256 and ReLU as activation;\n",
    "    - Dropout layer with probability of unit drop equal to 0.1;\n",
    "    - Final linear layer with size of output equals 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9fWUDdub06E",
    "outputId": "48d2995e-50eb-4ad1-bd05-1f842015335a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Build your model\n",
    "        self.conv1 = nn.Sequential(...)\n",
    "        self.conv2 = nn.Sequential(...)\n",
    "        self.conv3 = nn.Sequential(...)\n",
    "        self.linear1 = nn.Sequential(...)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagate x through the network\n",
    "        # Do not forget to flatten after the 3rd block\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = CustomModel().to(device)\n",
    "\n",
    "print(f'Device: {device}')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM_0vfc9b06F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3) Training pipeline upgrades: early stopping and LR scheduler\n",
    "\n",
    "Early Stopping is a form of regularization, used to stop training when a monitored metric has stopped improving.\n",
    "\n",
    "\n",
    "**Question: what kind of metric can we monitor? What's the benefit of using early stopping?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanila Pytorch doesn't contain early stopping (check [Pytorch Ignite](https://pytorch.org/ignite/generated/ignite.handlers.early_stopping.EarlyStopping.html) for 'official' solution), so we have to write it it from scratch. Although, sometimes it's useful to have such a custom tool which you can tune to your specific needs.\n",
    "\n",
    "**Task 3. Implement EarlyStopping class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SV3TQc7b06F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fill this class to stop when a certain value stop improving \n",
    "import operator\n",
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=5, min_delta=0, mode='min'):\n",
    "        '''\n",
    "        :param tolerance: number of epochs that the metric doesn't improve\n",
    "        :param min_delta: minimum improvement\n",
    "        :param mode: 'min' or 'max' to minimize or maximize the metric\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        You should keep these parameters,\n",
    "        define a counter of __call__ falses and the previous best value of metric\n",
    "        '''\n",
    "        self.early_stop = False\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, metric)->bool:\n",
    "        ''' This function should return True if `metric` is not improving for\n",
    "            'tolerance' calls\n",
    "        '''\n",
    "        if ...:\n",
    "            self.early_stop = True\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Let's look how different LR-schedulers work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Just a toy model \n",
    "class NullModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(1,1)\n",
    "    \n",
    "\n",
    "toy_model = NullModule()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def plot_lr(scheduler, name):\n",
    "    # Re-init for each scheduler\n",
    "    optimizer.param_groups[0]['lr'] = 0.01\n",
    "    optimizer.zero_grad()\n",
    "    toy_model.zero_grad()\n",
    "    lrs = []\n",
    "    step = 100\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel='Step', ylabel='LR value', title=name)\n",
    "\n",
    "    for i in range(step):\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if name == \"ReduceLROnPlateau\":\n",
    "            scheduler.step(lr) \n",
    "        else:\n",
    "            scheduler.step()\n",
    "        lrs.append(lr)\n",
    "\n",
    "    ax.plot(lrs)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# You can check https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "LRs = {\"ReduceLROnPlateau\": lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, \n",
    "                                                           patience=10, verbose=True,min_lr=0.001),\n",
    "       \"Step LR\": lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5),\n",
    "       \"Exponent LR\": lr_scheduler.ExponentialLR(optimizer, gamma=0.9),\n",
    "       \"Cyclic LR\":lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.2, \n",
    "                                         cycle_momentum=False, step_size_up=10)}\n",
    "\n",
    "for name, lr in LRs.items():\n",
    "    plot_lr(lr, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz8gdAFKb06F"
   },
   "source": [
    "# 4) Gather all together in training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc9nKg7Pb06F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time \n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    start_time = time()\n",
    "    correct = 0\n",
    "    iteration = 0\n",
    "    \n",
    "    bar = tqdm(train_loader)\n",
    "    for data, target in bar:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        # Get the index of the max log-probability\n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        iteration += 1\n",
    "        bar.set_postfix({\"Loss\": format(epoch_loss/iteration, '.6f')})\n",
    "\n",
    "    acc = 100. * correct / len(train_loader.dataset)\n",
    "    print(f'\\rTrain Epoch: {epoch}, elapsed time:{time()-start_time:.2f}s')\n",
    "    return epoch_loss, acc\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from copy import deepcopy\n",
    "\n",
    "# Define hyperparams\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "# Choose the LR you like\n",
    "scheduler = ...\n",
    "early_stopping = EarlyStopping(tolerance=7, mode='min')\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Use TensorBoard to check the progress of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJwFwepob06G",
    "outputId": "995538bd-0bb7-4424-97fc-011ebfc82956",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "def training(writing=False):\n",
    "    if writing:\n",
    "        writer = SummaryWriter(log_dir='runs/model')\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train(model, device, train_data_loader, criterion, optimizer, epoch)\n",
    "        # Update learning rate if needed\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        test_loss, test_acc = test(model, device, test_data_loader, criterion)\n",
    "        # Terminate training if loss stopped to decrease\n",
    "        if early_stopping(test_loss):\n",
    "            print('\\nEarly stopping\\n')\n",
    "            break\n",
    "        # Deep copy the weight of model if its accuracy is the best for now\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if writing:\n",
    "            writer.add_scalars('Loss',\n",
    "                            {\n",
    "                                'train': train_loss,\n",
    "                                'test': test_loss\n",
    "                            },\n",
    "                            epoch)\n",
    "\n",
    "            writer.add_scalars('Accuracy',\n",
    "                            {\n",
    "                                'train': train_acc,\n",
    "                                'test': test_acc\n",
    "                            },\n",
    "                            epoch)\n",
    "        else:\n",
    "            print(f\"Training accuracy {train_acc}, test accuracy {test_acc}\")\n",
    "            print(f\"Training loss {train_loss}, test loss {test_loss}\")\n",
    "        \n",
    "    torch.save(model.state_dict(), \"model.pt\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    if writing:\n",
    "        writer.close()\n",
    "        \n",
    "training()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
