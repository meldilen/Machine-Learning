{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab-12: Ensemble Learning\n",
    "\n",
    "In this lab, we will look at different ways to build ensemble models.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "* Bagging\n",
    "* Random Forests\n",
    "* AdaBoost\n",
    "\n",
    "\n",
    "Why ensemble learning? How does it help? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ensemble learning\n",
    "We will explore ensemble learning on the example of decision trees - we will see how ensembles can improve classification accuracy.\n",
    "\n",
    "Let's start from uploading MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(X[0].reshape((8,8)), cmap=\"gray\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(f\"label is {y[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Single decision tree\n",
    "\n",
    "First, we train a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single tree accuracy: 0.8501683501683501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "tree_score = accuracy_score(y_test, pred)\n",
    "print(\"Single tree accuracy:\", tree_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note the accuracy - it is around **0.85**.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "What is decreased by bagging? Variance or bias? How?\n",
    "\n",
    "Now let's improve it a bit by the means of bagging. We train a hundred of independent classifiers and make a prediction by majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for the 0th sample\n",
      "62 Trees predicted 3\n",
      "38 Trees predicted 8\n",
      "\n",
      "Predictions for the 1th sample\n",
      "100 Trees predicted 8\n",
      "\n",
      "Predictions for the 2th sample\n",
      "100 Trees predicted 2\n",
      "\n",
      "Predictions for the 3th sample\n",
      "100 Trees predicted 6\n",
      "\n",
      "Predictions for the 4th sample\n",
      "100 Trees predicted 6\n",
      "\n",
      "Bagging accuracy: 0.8804713804713805\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "\n",
    "n_trees = 100\n",
    "\n",
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new classifier and append it to the list\n",
    "    tree = DecisionTreeClassifier(random_state=i)\n",
    "    tree.fit(X_train, y_train)\n",
    "    classifiers.append(tree)\n",
    "\n",
    "# here we will store predictions for all samples and all base classifiers\n",
    "base_pred = np.zeros((X_test.shape[0], n_trees), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    # obtain the predictions from each tree\n",
    "    base_pred[:,i] = classifiers[i].predict(X_test)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f'Predictions for the {i}th sample')\n",
    "    sample_pred, sample_pred_count = np.unique(base_pred[i], return_counts=True)\n",
    "    for j in range(len(sample_pred)):\n",
    "        print(sample_pred_count[j],'Trees predicted', sample_pred[j])\n",
    "    print()\n",
    "\n",
    "# aggregate predictions by majority voting\n",
    "pred = mode(base_pred, axis=1)[0]\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Bagging accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now the accuracy grew up to **0.88**. Also, you can see that our classifiers\n",
    "return very similar results.\n",
    "\n",
    "Let's compare our bagging to SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747474747474747"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Why our performance is much lower then sklearn? \n",
    "\n",
    "\n",
    "### Random forest\n",
    "\n",
    "Compared to the simple bagging we've just implemented, random forest can show\n",
    "better results because base classifiers are much less correlated.\n",
    "\n",
    "At first, let's implement bootstrap sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2],\n",
       "        [ 9, 10, 11],\n",
       "        [ 3,  4,  5],\n",
       "        [ 0,  1,  2]]),\n",
       " array([0, 3, 1, 0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bootstrap(X, y):\n",
    "    # generate bootstrap indices and return data according to them\n",
    "    ind = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "    return X[ind,:], y[ind]\n",
    "\n",
    "\n",
    "# this is a test, will work if you are using np.random.randint() for indices generation\n",
    "np.random.seed(0)\n",
    "a = np.array(range(12)).reshape(4,3)\n",
    "b = np.array(range(4))\n",
    "bootstrap(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should get\n",
    "\n",
    "(array([[ 0,  1,  2], <br>\n",
    "&emsp;&emsp;&emsp;[ 9, 10, 11], <br>\n",
    "&emsp;&emsp;&emsp;[ 3,  4,  5], <br>\n",
    "&emsp;&emsp;&emsp;[ 0,  1,  2]]), <br>\n",
    "array([0, 3, 1, 0]))\n",
    "\n",
    "Now let's build a set of decision trees, each of them is trained on a bootstrap\n",
    "sampling from X and $\\sqrt d$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy: 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "for i in range(n_trees):\n",
    "    # train a new tree on sqrt(n_features) and bootstrapped data, append it to the list\n",
    "    base = DecisionTreeClassifier(max_features='sqrt', random_state=i)\n",
    "    x_boot, y_boot = bootstrap(X_train, y_train)\n",
    "    base.fit(x_boot, y_boot)\n",
    "    classifiers.append(base)\n",
    "\n",
    "base_pred = np.zeros((n_trees, X_test.shape[0]), dtype=\"int\")\n",
    "for i in range(n_trees):\n",
    "    base_pred[i,:] = classifiers[i].predict(X_test)\n",
    "\n",
    "pred = mode(base_pred, axis=0)[0].ravel()\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Random forest accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And now we got **0.98** accuracy, which is a significant improvement! Now you\n",
    "can see why it is so important to have diverse classifiers.\n",
    "\n",
    "---\n",
    "## Boosting\n",
    "\n",
    "How does boosting work?  \n",
    "\n",
    "For simplicity let's solve a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "data['target'][data['target']==0] = -1  # turn 0s to -1 to make the calculations easier\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's train a boosting model.\n",
    "\n",
    "We will have sample weights and tree weights. Initially all sample weights are equal. After that we will increase weight for complicated samples.\n",
    "\n",
    "Tree weight $w$ is computed using weighted error or $1 - accuracy$\n",
    "\n",
    "$w_t = \\frac12 log(\\frac{1-weighted\\_error_t}{weighted\\_error_t})$ for each base classifier.\n",
    "\n",
    "For correct samples weights will be decreased $e^w$ times, and for incorrect classified samples increased  $e^w$ times. After this changes we normalize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_trees = 3\n",
    "tree_weights = np.zeros(n_trees)\n",
    "classifiers = []\n",
    "train_samples = X_train.shape[0]\n",
    "# initialize sample weights\n",
    "sample_weights = np.ones(train_samples) / train_samples\n",
    "for i in range(n_trees):\n",
    "    clf = DecisionTreeClassifier(min_samples_leaf=3, random_state=42)\n",
    "    clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    pred = clf.predict(X_train)\n",
    "    acc = accuracy_score(y_train, pred, sample_weight=sample_weights)\n",
    "    # Calculate tree weight\n",
    "    w = 1/2*np.log(acc / (1 - acc))\n",
    "    tree_weights[i] = w\n",
    "    classifiers.append(clf)\n",
    "    # Update sample weights\n",
    "    for j in range(train_samples):\n",
    "        if pred[j] != y[j]:\n",
    "            sample_weights[j] = sample_weights[j] * np.exp(w)\n",
    "        else:\n",
    "            sample_weights[j] = sample_weights[j] * np.exp((-w))\n",
    "    # normalize the weights\n",
    "    sample_weights = sample_weights / np.sum(sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use trees voting to calculate final predictions. Since we have a binary classification, the prediction will be calculated as follows:\n",
    "\n",
    "$\\hat{y} = sign(\\sum_{t=1}^{T}(w_t f_t(x)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "n_test = X_test.shape[0]\n",
    "\n",
    "pred = np.zeros(n_test)\n",
    "\n",
    "# Aggregate the  predictions\n",
    "for i in range(n_trees):\n",
    "    pred += tree_weights[i] * classifiers[i].predict(X_test)\n",
    "\n",
    "for i in range(n_test):\n",
    "    pred[i] = 1 if pred[i] > 0 else -1\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(\"Boosting accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)\n",
    "Sklearn has many ensemble [methods](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100\n",
    ")\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print('AdaBoost accuracy:', clf.score(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
